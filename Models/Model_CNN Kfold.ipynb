{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Preprocessed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3908_7_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3418_functional.nii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4104_4_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4104_6_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100_4_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3917_4_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>3460_4_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3444_functional.nii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3237_2_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3908_6_functional.nii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename  target\n",
       "0    3908_7_functional.nii       0\n",
       "1      3418_functional.nii       1\n",
       "2    4104_4_functional.nii       0\n",
       "3    4104_6_functional.nii       0\n",
       "4    4100_4_functional.nii       0\n",
       "..                     ...     ...\n",
       "119  3917_4_functional.nii       0\n",
       "120  3460_4_functional.nii       0\n",
       "121    3444_functional.nii       1\n",
       "122  3237_2_functional.nii       0\n",
       "123  3908_6_functional.nii       0\n",
       "\n",
       "[124 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = glob.glob('data/all_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing an array to store images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading inages\n",
    "#### Shape Correction (taking middle 30 slides for each Scan)\n",
    "#### Appending the images into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/all_data/3908_7_functional.nii\n",
      "data/all_data/3418_functional.nii\n",
      "data/all_data/4104_4_functional.nii\n",
      "data/all_data/4104_6_functional.nii\n",
      "data/all_data/4100_4_functional.nii\n",
      "data/all_data/4055_functional.nii\n",
      "data/all_data/3666_functional.nii\n",
      "data/all_data/3237_3_functional.nii\n",
      "data/all_data/3551_2_functional.nii\n",
      "data/all_data/4100_2_functional.nii\n",
      "data/all_data/3201_3_functional.nii\n",
      "data/all_data/3433_functional.nii\n",
      "data/all_data/3431_functional.nii\n",
      "data/all_data/3448_functional.nii\n",
      "data/all_data/4102_functional.nii\n",
      "data/all_data/3443_functional.nii\n",
      "data/all_data/4104_2_functional.nii\n",
      "data/all_data/4111_functional.nii\n",
      "data/all_data/3225_functional.nii\n",
      "data/all_data/3551_3_functional.nii\n",
      "data/all_data/4094_functional.nii\n",
      "data/all_data/4116_5_functional.nii\n",
      "data/all_data/3901_3_functional.nii\n",
      "data/all_data/3917_1_functional.nii\n",
      "data/all_data/4116_2_functional.nii\n",
      "data/all_data/4117_functional.nii\n",
      "data/all_data/3132_functional.nii\n",
      "data/all_data/3908_2_functional.nii\n",
      "data/all_data/3235_1_functional.nii\n",
      "data/all_data/3468_3_functional.nii\n",
      "data/all_data/4135_functional.nii\n",
      "data/all_data/4104_1_functional.nii\n",
      "data/all_data/4106_functional.nii\n",
      "data/all_data/3237_1_functional.nii\n",
      "data/all_data/3908_5_functional.nii\n",
      "data/all_data/4093_functional.nii\n",
      "data/all_data/3201_2_functional.nii\n",
      "data/all_data/3124_functional.nii\n",
      "data/all_data/4085_4_functional.nii\n",
      "data/all_data/3900_functional.nii\n",
      "data/all_data/3230_functional.nii\n",
      "data/all_data/3435_functional.nii\n",
      "data/all_data/4104_9_functional.nii\n",
      "data/all_data/4085_3_functional.nii\n",
      "data/all_data/4098_functional.nii\n",
      "data/all_data/3237_4_functional.nii\n",
      "data/all_data/3901_1_functional.nii\n",
      "data/all_data/3453_4_functional.nii\n",
      "data/all_data/4083_functional.nii\n",
      "data/all_data/4105_4_functional.nii\n",
      "data/all_data/3445_functional.nii\n",
      "data/all_data/4116_1_functional.nii\n",
      "data/all_data/3460_3_functional.nii\n",
      "data/all_data/4104_3_functional.nii\n",
      "data/all_data/3911_functional.nii\n",
      "data/all_data/4113_functional.nii\n",
      "data/all_data/3664_functional.nii\n",
      "data/all_data/3468_4_functional.nii\n",
      "data/all_data/3908_8_functional.nii\n",
      "data/all_data/3423_functional.nii\n",
      "data/all_data/3453_2_functional.nii\n",
      "data/all_data/3234_functional.nii\n",
      "data/all_data/3460_1_functional.nii\n",
      "data/all_data/3551_4_functional.nii\n",
      "data/all_data/3228_functional.nii\n",
      "data/all_data/3665_functional.nii\n",
      "data/all_data/4091_functional.nii\n",
      "data/all_data/4057_functional.nii\n",
      "data/all_data/3422_functional.nii\n",
      "data/all_data/4105_3_functional.nii\n",
      "data/all_data/4116_3_functional.nii\n",
      "data/all_data/3453_3_functional.nii\n",
      "data/all_data/3910_functional.nii\n",
      "data/all_data/3235_2_functional.nii\n",
      "data/all_data/3661_functional.nii\n",
      "data/all_data/3201_1_functional.nii\n",
      "data/all_data/3901_2_functional.nii\n",
      "data/all_data/3201_4_functional.nii\n",
      "data/all_data/3914_functional.nii\n",
      "data/all_data/4058_functional.nii\n",
      "data/all_data/4082_functional.nii\n",
      "data/all_data/3125_functional.nii\n",
      "data/all_data/3460_2_functional.nii\n",
      "data/all_data/3551_1_functional.nii\n",
      "data/all_data/3453_1_functional.nii\n",
      "data/all_data/4105_1_functional.nii\n",
      "data/all_data/3436_functional.nii\n",
      "data/all_data/4100_1_functional.nii\n",
      "data/all_data/4085_1_functional.nii\n",
      "data/all_data/4108_functional.nii\n",
      "data/all_data/4105_2_functional.nii\n",
      "data/all_data/3908_1_functional.nii\n",
      "data/all_data/3220_functional.nii\n",
      "data/all_data/3662_1_functional.nii\n",
      "data/all_data/4104_8_functional.nii\n",
      "data/all_data/3223_functional.nii\n",
      "data/all_data/3123_functional.nii\n",
      "data/all_data/3662_2_functional.nii\n",
      "data/all_data/3409_functional.nii\n",
      "data/all_data/3908_4_functional.nii\n",
      "data/all_data/4096_functional.nii\n",
      "data/all_data/4100_3_functional.nii\n",
      "data/all_data/3468_2_functional.nii\n",
      "data/all_data/4051_functional.nii\n",
      "data/all_data/4104_7_functional.nii\n",
      "data/all_data/4105_5_functional.nii\n",
      "data/all_data/3459_functional.nii\n",
      "data/all_data/4081_functional.nii\n",
      "data/all_data/3908_3_functional.nii\n",
      "data/all_data/4100_5_functional.nii\n",
      "data/all_data/3905_functional.nii\n",
      "data/all_data/4104_5_functional.nii\n",
      "data/all_data/3917_2_functional.nii\n",
      "data/all_data/4085_2_functional.nii\n",
      "data/all_data/3468_1_functional.nii\n",
      "data/all_data/3916_functional.nii\n",
      "data/all_data/4116_4_functional.nii\n",
      "data/all_data/3901_4_functional.nii\n",
      "data/all_data/3917_3_functional.nii\n",
      "data/all_data/3917_4_functional.nii\n",
      "data/all_data/3460_4_functional.nii\n",
      "data/all_data/3444_functional.nii\n",
      "data/all_data/3237_2_functional.nii\n",
      "data/all_data/3908_6_functional.nii\n"
     ]
    }
   ],
   "source": [
    "for f in range(len(ff)):\n",
    "    a = nib.load(ff[f])\n",
    "    print(ff[f])\n",
    "    l1,l2,l3=a.shape\n",
    "    #print(l1,l2,l3)\n",
    "    a = a.get_data()\n",
    "    #print(a)\n",
    "    \n",
    "    z1= int(l3/2) -15\n",
    "    z2= int(l3/2) +15\n",
    "    a = a[:,:,z1:z2]\n",
    "    #print(a)\n",
    "    #for i in range (len(ff)):\n",
    "    images.append((a[:,:,:]))\n",
    "    #print (a.shape)\n",
    "#print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagess=np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 128, 128, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagess.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.max(imagess)\n",
    "mi = np.min(imagess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32767.0 -607.0\n"
     ]
    }
   ],
   "source": [
    "print(m, mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = (images - mi) / (m - mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(images), np.max(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing depedencies for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPool3D, Flatten, Dense\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "import plotly.graph_objs as go\n",
    "from matplotlib.pyplot import cm\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json,Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import itertools\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D, Input, Convolution2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Function for\n",
    "##### Saving MetricsCheckpoint\n",
    "##### Plotting Confusion Matrix\n",
    "##### Plotting Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions  Learning Curves and Confusion Matrix\n",
    "\n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "def plotKerasLearningCurve():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of epochs')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./accuracy_curve.png')\n",
    "    #plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./loss_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 128, 128, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagess.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading output Labels\n",
    "##### PD=1, Control=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "119    0\n",
      "120    0\n",
      "121    1\n",
      "122    0\n",
      "123    0\n",
      "Name: target, Length: 124, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv')\n",
    "y=data.target\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K=4 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 93 samples, validate on 31 samples\n",
      "Epoch 1/15\n",
      " - 10s - loss: 3.7280 - acc: 0.5269 - val_loss: 12.2680 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58065, saving model to pd_cnn.h5\n",
      "Epoch 2/15\n",
      " - 8s - loss: 3.2037 - acc: 0.6022 - val_loss: 1.8700 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.58065\n",
      "Epoch 3/15\n",
      " - 8s - loss: 1.7424 - acc: 0.6989 - val_loss: 2.8250 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.58065\n",
      "Epoch 4/15\n",
      " - 8s - loss: 1.3966 - acc: 0.6559 - val_loss: 1.1399 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58065 to 0.74194, saving model to pd_cnn.h5\n",
      "Epoch 5/15\n",
      " - 8s - loss: 1.2378 - acc: 0.6667 - val_loss: 1.9149 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.74194\n",
      "Epoch 6/15\n",
      " - 8s - loss: 1.0470 - acc: 0.6989 - val_loss: 1.4056 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.74194\n",
      "Epoch 7/15\n",
      " - 8s - loss: 0.8474 - acc: 0.7097 - val_loss: 0.6982 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.74194\n",
      "Epoch 8/15\n",
      " - 8s - loss: 0.5648 - acc: 0.8172 - val_loss: 0.6474 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74194\n",
      "Epoch 9/15\n",
      " - 8s - loss: 0.7117 - acc: 0.7312 - val_loss: 0.4257 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74194 to 0.80645, saving model to pd_cnn.h5\n",
      "Epoch 10/15\n",
      " - 8s - loss: 0.9891 - acc: 0.7742 - val_loss: 0.2146 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80645 to 0.93548, saving model to pd_cnn.h5\n",
      "Epoch 11/15\n",
      " - 9s - loss: 0.4310 - acc: 0.8172 - val_loss: 0.4502 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93548\n",
      "Epoch 12/15\n",
      " - 8s - loss: 0.3753 - acc: 0.8602 - val_loss: 0.1518 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.93548 to 0.96774, saving model to pd_cnn.h5\n",
      "Epoch 13/15\n",
      " - 8s - loss: 0.3870 - acc: 0.8710 - val_loss: 0.1549 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.96774\n",
      "Epoch 14/15\n",
      " - 8s - loss: 0.2703 - acc: 0.8925 - val_loss: 0.1998 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.96774\n",
      "Epoch 15/15\n",
      " - 8s - loss: 0.4854 - acc: 0.8065 - val_loss: 0.3469 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96774\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keras CNN  # 1  - accuracy: 0.8709677457809448 \n",
      "\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "Train on 93 samples, validate on 31 samples\n",
      "Epoch 1/15\n",
      " - 10s - loss: 3.2147 - acc: 0.5376 - val_loss: 17.9316 - val_acc: 0.4516\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.45161, saving model to pd_cnn.h5\n",
      "Epoch 2/15\n",
      " - 9s - loss: 3.6816 - acc: 0.4946 - val_loss: 1.3436 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.45161 to 0.61290, saving model to pd_cnn.h5\n",
      "Epoch 3/15\n",
      " - 8s - loss: 1.7742 - acc: 0.6559 - val_loss: 7.7711 - val_acc: 0.4516\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61290\n",
      "Epoch 4/15\n",
      " - 8s - loss: 1.4888 - acc: 0.7312 - val_loss: 1.5589 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61290\n",
      "Epoch 5/15\n",
      " - 8s - loss: 0.9432 - acc: 0.7419 - val_loss: 3.6387 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61290\n",
      "Epoch 6/15\n",
      " - 8s - loss: 1.7748 - acc: 0.5914 - val_loss: 0.8585 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.61290 to 0.70968, saving model to pd_cnn.h5\n",
      "Epoch 7/15\n",
      " - 8s - loss: 0.9758 - acc: 0.7312 - val_loss: 1.1367 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.70968 to 0.74194, saving model to pd_cnn.h5\n",
      "Epoch 8/15\n",
      " - 8s - loss: 0.5259 - acc: 0.8817 - val_loss: 1.1178 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74194\n",
      "Epoch 9/15\n",
      " - 8s - loss: 0.7415 - acc: 0.7957 - val_loss: 0.9916 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74194\n",
      "Epoch 10/15\n",
      " - 8s - loss: 0.7873 - acc: 0.7742 - val_loss: 0.7426 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.74194 to 0.80645, saving model to pd_cnn.h5\n",
      "Epoch 11/15\n",
      " - 8s - loss: 0.6808 - acc: 0.7634 - val_loss: 1.0105 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80645\n",
      "Epoch 12/15\n",
      " - 8s - loss: 0.4533 - acc: 0.8172 - val_loss: 0.7914 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80645\n",
      "Epoch 13/15\n",
      " - 9s - loss: 0.2741 - acc: 0.8710 - val_loss: 0.6894 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.80645 to 0.83871, saving model to pd_cnn.h5\n",
      "Epoch 14/15\n",
      " - 8s - loss: 0.4183 - acc: 0.8817 - val_loss: 0.6565 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83871\n",
      "Epoch 15/15\n",
      " - 8s - loss: 0.4423 - acc: 0.8387 - val_loss: 0.6605 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.83871 to 0.87097, saving model to pd_cnn.h5\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keras CNN  # 2  - accuracy: 0.8709677457809448 \n",
      "\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "Train on 93 samples, validate on 31 samples\n",
      "Epoch 1/15\n",
      " - 10s - loss: 2.1279 - acc: 0.5914 - val_loss: 1.5035 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70968, saving model to pd_cnn.h5\n",
      "Epoch 2/15\n",
      " - 8s - loss: 2.4736 - acc: 0.5806 - val_loss: 4.3878 - val_acc: 0.4516\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.70968\n",
      "Epoch 3/15\n",
      " - 8s - loss: 1.7680 - acc: 0.6022 - val_loss: 3.5825 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.70968\n",
      "Epoch 4/15\n",
      " - 8s - loss: 1.6294 - acc: 0.6452 - val_loss: 2.3356 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.70968\n",
      "Epoch 5/15\n",
      " - 8s - loss: 1.0109 - acc: 0.7849 - val_loss: 0.3854 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70968 to 0.83871, saving model to pd_cnn.h5\n",
      "Epoch 6/15\n",
      " - 8s - loss: 0.5471 - acc: 0.8065 - val_loss: 0.3524 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.83871\n",
      "Epoch 7/15\n",
      " - 8s - loss: 0.6702 - acc: 0.7849 - val_loss: 0.3687 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83871 to 0.87097, saving model to pd_cnn.h5\n",
      "Epoch 8/15\n",
      " - 8s - loss: 0.5578 - acc: 0.7742 - val_loss: 0.3505 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87097\n",
      "Epoch 9/15\n",
      " - 8s - loss: 0.5349 - acc: 0.8710 - val_loss: 0.3211 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.87097 to 0.90323, saving model to pd_cnn.h5\n",
      "Epoch 10/15\n",
      " - 8s - loss: 0.3061 - acc: 0.9032 - val_loss: 0.3653 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90323\n",
      "Epoch 11/15\n",
      " - 8s - loss: 0.3168 - acc: 0.8710 - val_loss: 0.4108 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90323\n",
      "Epoch 12/15\n",
      " - 8s - loss: 0.2432 - acc: 0.8817 - val_loss: 0.3046 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90323\n",
      "Epoch 13/15\n",
      " - 8s - loss: 0.2871 - acc: 0.9140 - val_loss: 0.2760 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90323\n",
      "Epoch 14/15\n",
      " - 8s - loss: 0.2029 - acc: 0.8925 - val_loss: 0.3313 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90323\n",
      "Epoch 15/15\n",
      " - 8s - loss: 0.2543 - acc: 0.8925 - val_loss: 0.4242 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90323\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keras CNN  # 3  - accuracy: 0.8064516186714172 \n",
      "\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "Train on 93 samples, validate on 31 samples\n",
      "Epoch 1/15\n",
      " - 10s - loss: 4.5677 - acc: 0.4624 - val_loss: 22.3724 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.32258, saving model to pd_cnn.h5\n",
      "Epoch 2/15\n",
      " - 8s - loss: 3.0974 - acc: 0.4839 - val_loss: 6.9407 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.32258\n",
      "Epoch 3/15\n",
      " - 8s - loss: 1.7256 - acc: 0.5806 - val_loss: 3.6868 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.32258 to 0.67742, saving model to pd_cnn.h5\n",
      "Epoch 4/15\n",
      " - 8s - loss: 2.3299 - acc: 0.5914 - val_loss: 3.3735 - val_acc: 0.3871\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67742\n",
      "Epoch 5/15\n",
      " - 8s - loss: 1.6369 - acc: 0.6667 - val_loss: 7.1608 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67742\n",
      "Epoch 6/15\n",
      " - 8s - loss: 2.1920 - acc: 0.6237 - val_loss: 8.6716 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67742\n",
      "Epoch 7/15\n",
      " - 8s - loss: 1.7717 - acc: 0.5806 - val_loss: 1.1922 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.67742 to 0.70968, saving model to pd_cnn.h5\n",
      "Epoch 8/15\n",
      " - 8s - loss: 1.3066 - acc: 0.6452 - val_loss: 0.3495 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.70968 to 0.87097, saving model to pd_cnn.h5\n",
      "Epoch 9/15\n",
      " - 8s - loss: 1.7141 - acc: 0.6667 - val_loss: 0.5590 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87097\n",
      "Epoch 10/15\n",
      " - 8s - loss: 1.1351 - acc: 0.6882 - val_loss: 2.3969 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87097\n",
      "Epoch 11/15\n",
      " - 8s - loss: 0.7323 - acc: 0.7419 - val_loss: 1.1070 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87097\n",
      "Epoch 12/15\n",
      " - 8s - loss: 0.6720 - acc: 0.7957 - val_loss: 0.7028 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87097\n",
      "Epoch 13/15\n",
      " - 8s - loss: 0.4859 - acc: 0.7957 - val_loss: 0.4782 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87097\n",
      "Epoch 00013: early stopping\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keras CNN  # 4  - accuracy: 0.8709677457809448 \n",
      "\n",
      ".................................................\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 13\n",
    "np.random.seed(13)\n",
    "cvscores = []\n",
    "y1= keras.utils.to_categorical(y, 2)\n",
    "\n",
    " n_split = 4\n",
    "count=1\n",
    "for train, test in KFold(n_split).split(imagess):\n",
    "    x_traincnn, x_testcnn=imagess[train],imagess[test]\n",
    "    y_traincnn, y_testcnn=y1[train],y1[test]\n",
    "    ## input layer\n",
    "    input_layer = Input((128, 128, 30))\n",
    "\n",
    "    ## convolutional layers\n",
    "    conv_layer1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu',padding='same')(input_layer)\n",
    "    conv_layer2 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu',padding='same')(conv_layer1)\n",
    "\n",
    "    ## add max pooling to obtain the most imformatic features\n",
    "    pooling_layer1 = MaxPooling2D(pool_size=(2, 2))(conv_layer2)\n",
    "\n",
    "    conv_layer3 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(pooling_layer1)\n",
    "    conv_layer4 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(conv_layer3)\n",
    "    pooling_layer2 = MaxPooling2D(pool_size=(2, 2))(conv_layer4)\n",
    "\n",
    "    ## perform batch normalization on the convolution outputs before feeding it to MLP architecture\n",
    "    pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "    flatten_layer = Flatten()(pooling_layer2)\n",
    "\n",
    "    ## create an MLP architecture with dense layers : 2048 -> 512 -> 2\n",
    "    ## add dropouts to avoid overfitting / perform regularization\n",
    "    dense_layer1 = Dense(units=2048, activation='relu')(flatten_layer)\n",
    "    dense_layer1 = Dropout(0.5)(dense_layer1)\n",
    "    dense_layer2 = Dense(units=512, activation='relu')(dense_layer1)\n",
    "    dense_layer2 = Dropout(0.5)(dense_layer2)\n",
    "    output_layer = Dense(units=2, activation='softmax')(dense_layer2)\n",
    "\n",
    "    ## define the model with input layer and output layer\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "    checkpointer = ModelCheckpoint('pd_cnn.h5',monitor='val_acc', verbose=1, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adadelta(lr=0.1), metrics=['acc'])\n",
    "    # Without data augmentation\n",
    "    history = model.fit(x_traincnn, y_traincnn, batch_size = 20, epochs = 15, validation_data = (x_testcnn, y_testcnn), verbose = 2, callbacks = [MetricsCheckpoint('logs'),earlystopper,learning_rate_reduction,checkpointer])\n",
    "    score = model.evaluate(x_testcnn, y_testcnn, verbose=0)\n",
    "    if count==1:\n",
    "        accu1=score[1]\n",
    "    elif count==2:\n",
    "        accu2=score[1]\n",
    "    elif count==3:\n",
    "        accu3=score[1]\n",
    "    elif count==4:\n",
    "        accu4=score[1]\n",
    "    #elif count==5:\n",
    "        #accu5=score[1]\n",
    "    print('.................................................\\n\\n\\n')\n",
    "    print('\\nKeras CNN  #', count,' - accuracy:', score[1],'\\n')\n",
    "    print('.................................................\\n\\n\\n')\n",
    "    count=count+1\n",
    "    y_pred = model.predict(x_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8709677457809448\n",
      "0.8709677457809448\n",
      "0.8064516186714172\n",
      "0.8709677457809448\n",
      "0.8548387140035629\n"
     ]
    }
   ],
   "source": [
    "print(accu1)\n",
    "print(accu2)\n",
    "print(accu3)\n",
    "print(accu4)\n",
    "#print(accu5)\n",
    "print((accu1+accu2+accu3+accu4)/4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
